{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2fQg_GX1Fke",
        "outputId": "bc65fd02-81c5-41f0-ba22-d68e2298143b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.21.6)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.5.18.1)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "LqKsJ_O20med"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import mxnet \n",
        "from mxnet import autograd, np, npx\n",
        "import random\n",
        "npx.set_np()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Dataset"
      ],
      "metadata": {
        "id": "kfg1kRx00pxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def synthetic_data(w, b, num_examples): \n",
        "  \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
        "  X = np.random.normal(0, 1, (num_examples, len(w)))\n",
        "  y = np.dot(X, w) + b\n",
        "  y += np.random.normal(0, 0.01, y.shape)\n",
        "  return X, y.reshape((-1, 1))"
      ],
      "metadata": {
        "id": "EvyjHGMY0zUF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_w = np.array([2, -3.4])\n",
        "true_b = 4.2\n",
        "features, labels = synthetic_data(true_w, true_b, 1000)"
      ],
      "metadata": {
        "id": "j1LRmBWf1Wdc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Dataset"
      ],
      "metadata": {
        "id": "8SGXsAA-1t6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_iter(batch_size, features, labels):\n",
        "  num_examples = len(features)\n",
        "  indices = list(range(num_examples))\n",
        "  # The examples are read at random, in no particular order\n",
        "\n",
        "  random.shuffle(indices)\n",
        "  for i in range(0, num_examples, batch_size):\n",
        "    batch_indices = np.array(\n",
        "                             indices[i: min(i + batch_size, num_examples)])\n",
        "  yield features[batch_indices], labels[batch_indices]"
      ],
      "metadata": {
        "id": "zkjBnAkX1jSJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init model parameters, model, loss fn, opt fn"
      ],
      "metadata": {
        "id": "uEQUDyaU1_uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.random.normal(0, 0.01, (2, 1))\n",
        "b = np.zeros(1)\n",
        "w.attach_grad()\n",
        "b.attach_grad()"
      ],
      "metadata": {
        "id": "te_xis-N16qd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linreg(X, w, b): \n",
        "  \"\"\"The linear regression model.\"\"\"\n",
        "  return np.dot(X, w) + b\n",
        "\n",
        "def squared_loss(y_hat, y): \n",
        "  \"\"\"Squared loss.\"\"\"\n",
        "  return (y_hat - y.reshape(y_hat.shape))**2 / 2\n",
        "\n",
        "def sgd(params, lr, batch_size): \n",
        "  \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "  for param in params:\n",
        "    param[:] = param - lr * param.grad / batch_size"
      ],
      "metadata": {
        "id": "2iIYHfBF2CaZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "iBk8mWZ-3RrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.03\n",
        "num_epochs = 300\n",
        "net = linreg\n",
        "loss = squared_loss\n",
        "batch_size= 1000"
      ],
      "metadata": {
        "id": "S_ajl2dw3Qk3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for X, y in data_iter(batch_size, features, labels):\n",
        "    with autograd.record():\n",
        "      l = loss(net(X, w, b), y) # Minibatch loss in `X` and `y`\n",
        "  # Because `l` has a shape (`batch_size`, 1) and is not a scalar\n",
        "  # variable, the elements in `l` are added together to obtain a new\n",
        "  # variable, on which gradients with respect to [`w`, `b`] are computed\n",
        "\n",
        "    l.backward()\n",
        "    sgd([w, b], lr, batch_size) # Update parameters using their gradient\n",
        "  train_l = loss(net(features, w, b), labels)\n",
        "  if epoch%30 == 0:\n",
        "    print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzgaFVyX3UyE",
        "outputId": "e8d1fa36-9666-4334-a11f-5b328f4c89ee"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss 15.166262\n",
            "epoch 31, loss 2.565142\n",
            "epoch 61, loss 0.434754\n",
            "epoch 91, loss 0.073871\n",
            "epoch 121, loss 0.012613\n",
            "epoch 151, loss 0.002192\n",
            "epoch 181, loss 0.000416\n",
            "epoch 211, loss 0.000112\n",
            "epoch 241, loss 0.000060\n",
            "epoch 271, loss 0.000051\n"
          ]
        }
      ]
    }
  ]
}