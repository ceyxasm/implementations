# -*- coding: utf-8 -*-
"""Lab_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A4QjcDpUm_2RntnAk_D4tm3_cfqXBYRB

**Problem: 1**
"""

#a
import pandas as pd
data=pd.read_csv('/content/drive/MyDrive/Classroom/Pattern Recognition and Machine Learning - 2022 Winter Semester/Lab/lab_1/Milk Grading (1).csv')
l=[]
features=data.keys()
for i in features:
  for j in data[i]:
    l.append(j)
print('csv to list :',l)


#b converting input to number
n=int(input())
print('number :',n)

#c string to datetime
from datetime import datetime
date_time = "Nov 27 2000 1:45PM"
obj = datetime.strptime(date_time, '%b %d %Y %I:%M%p')
print('\ndate :', obj.date())
print('time :',obj.time())

#d to call external command in py
import os
os.system("dir *.md")

#e. count occurances of an element in list
l=[1, 1, 1, 1, 1, 2, 3, 4, 5, 6, 6, 6, 11, 11, 13]
print('count of 1: ',l.count(1))
print('count of 6: ', l.count(6))
print('count of 13: ', l.count(13))

#f. flattening lists
l=[[1, 2, 3], ['x', 'y', 'z', 'a'], [3.3, 6.9, 4.20]]
l=[i for inner_list in l for i in inner_list]
print('\n',l)

#g. merging dicts
a= { 'name':'abu',
      'from':'lucknow',
       'loves': 'cats'}
b={ 'hates': '8am classes and deadlines',
    'number of cats': 0,
     'height in cm': 172}
c=a.copy()
c.update(b)
print('\n', c)

#h removing repititive elements from list
l=[1, 1, 1, 1, 1, 2, 3, 4, 5, 6, 6, 6, 11, 11, 13]
l=list(set(l))
print('\n', l,'\n')

#i checking for a key in dict
key= ['gf', 'loves', 'name', 'height in inch']
for i in key:
  print(i, i in c.keys())

"""**Problem: 2**"""

import numpy as np

a=np.arange(1,10)
b=np.arange(10,19)
a.resize((3,3))
b.resize((3,3,))
print(a,'\n\n', b,'\n\n')

#a
print('first row of first matrix: ', a[0, :], '\n')
#b
print('secodn column of second matrix', b[:, 1],'\n')
#c
print('a.b:\n', a.dot(b),'\n')
#d
print('elementwise multiplication:\n', a*b,'\n')
#e
at=a.T
bt=b.T
print('dot product of columns')
for i in at:
  for j in bt:
    print('dot product between ', i, ' and  ', j, 'is: ', i.dot(j))

"""**Problem: 3**"""

##problem 3
import pandas as pd

data=pd.read_csv('/content/drive/MyDrive/Classroom/Pattern Recognition and Machine Learning - 2022 Winter Semester/Lab/lab_1/Cars93.csv')
data.info()
data.head()

#i Assign a type to each of the following features
'''
Ordinal Scale: Type, AirBags
Nominal Scale: Manufacturer, Model, Drivetrain, Man.trans.avail, Origin
Ratio Scale: Min.Price, Price, Max.Price, Cylinders, EngineSize, Horsepower, RPM, Rev.per.mile, Fuel.tank.capacity, Passengers, Length, Wheelbase, Width, Turn.circle, Rear.seat.room, Luggage.room, Weight,
Interval Scale: MPG.city, MPG.highway
'''

#ii .As the rear seat room and lugage room are missing for some instances, they need to be taken care of
## As ablout 11 instances havee missing lugage room, dropping the models is not feasible, rather we are going to use mean.
data.dropna(inplace=True)
data.head()

#iii removing noise
'''inspection of the dataset shows that there is some noise in model names '''
cl=['Model'] #columns with noise in them
for i in cl:
  val=data[i] #values of different models
  for j in data.index.values:
    if val[j].isdigit(): #if model is an integer, drop the corresponding row
      data.drop(j, inplace=True)


data.reset_index(inplace=True)
data.drop('index', inplace=True, axis=1)
data.head()

#iv categorically encoding attributes

from sklearn.preprocessing import OneHotEncoder
cat_encoder = OneHotEncoder(sparse=False)
nominal_features=[ 'Manufacturer', 'Model', 'DriveTrain', 'Man.trans.avail', 'Origin'] 
data_cat_en = cat_encoder.fit_transform(data[nominal_features])

for row in data_cat_en:
  for val in range(len(row)):
    col='col'+ str(val+1)
    data[col]=0
  for i in range(len(data_cat_en)):
    data[col][i]=data_cat_en[i, val]

data.drop(nominal_features, inplace=True, axis=1)
data.head()

#v normalizing our dataset
from sklearn.preprocessing import StandardScaler
scaler= StandardScaler()
num_features=['Min.Price','Price'	,'Max.Price'	,'MPG.city'	,'MPG.highway' ,'Cylinders'	,'EngineSize'	,'Horsepower'	,'RPM'	,'Rev.per.mile' ,'Fuel.tank.capacity'	,'Passengers',	'Length'	,'Wheelbase'	,'Width'	,'Turn.circle'	,'Rear.seat.room'	,'Luggage.room'	,'Weight']
scaler.fit(data[num_features])

#vi train-test-val split
from sklearn.model_selection import train_test_split
train_set2, test_set = train_test_split(data, test_size=0.1, random_state=42)
train_set, val_set= train_test_split(data, test_size=0.222222, random_state=42)

"""**Problem: 4**"""

import matplotlib.pyplot as plt

#a
x=np.arange(-10,11)
y= 5*x+4
plt.plot(x,y)
plt.title('y= 5*x+4')
plt.show()

#b
x=np.linspace(10,100,1000)
y=np.log(x)
plt.plot(x,y)
plt.title('ln(x)')
plt.show()

#c
x=np.arange(-10, 11)
y=x**2
plt.title('y=x^2')
plt.plot(x,y)
plt.show()

"""**Problem 5**"""

## copied from shared collab file
#Import the Necessary Python Libraries and Components
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import LogisticRegression as LR
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import precision_score as ps
from sklearn.metrics import recall_score as rs
from sklearn.metrics import f1_score as f1s
from sklearn.metrics import accuracy_score as acc

#To Disable Convergence Warnings (For Custom Training)
from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

data=pd.read_csv('/content/drive/MyDrive/Classroom/Pattern Recognition and Machine Learning - 2022 Winter Semester/Lab/lab_1/data.csv')

#Convert the String Labels into easily-interpretable Numerics
condition_M = data.diagnosis == "M"
condition_B = data.diagnosis == "B"

data.loc[condition_M,"diagnosis"]=0
data.loc[condition_B,"diagnosis"]=1

##Converting Dataframe into Numpy Arrays (Features and Labels)
Y = data.diagnosis.to_numpy().astype('int')                                     # Labels

X_data = data.drop(columns=["id","diagnosis","Unnamed: 32"])
X = X_data.to_numpy() 

#data split
user_prompt = 0.3
user_enable = False

x_train,x_test,y_train,y_test = tts(X,Y,test_size=user_prompt,shuffle=user_enable)

##model training and prediction
logistic_model = LR()
logistic_model.fit(x_train,y_train)
logistic_pred = logistic_model.predict(x_test)

decision_model = DTC()
decision_model.fit(x_train,y_train)
decision_pred = decision_model.predict(x_test)

inbuilt_matrix_logistic = cm(y_test,logistic_pred)
inbuilt_matrix_decision = cm(y_test,decision_pred)

print("Confusion Matrix for Logistic Regression-based Predictions =>")
print(inbuilt_matrix_logistic)
print("Confusion Matrix for Decision Tree-based Predictions =>")
print(inbuilt_matrix_decision)

def confusion_matrix(y, pred):
  cm=np.array([[0,0],[0,0]])
  for i in range(len(y)):
    if (y[i]==0 and pred[i]==0): ##when our false class is correctly predicted false- TN
      cm[0][0]+=1
    if (y[i]==0 and pred[i]==1): ##when out false class falsely predicted true- FP
      cm[0][1]+=1
    if (y[i]==1 and pred[i]==0): ##when our true class falsely predicted flase- FN
      cm[1][0]+=1
    if (y[i]==1 and pred[i]==1): ##when our true class is correctly predicted true- TP
      cm[1][1]+=1
  return cm

log_cm=confusion_matrix(y_test, logistic_pred)
print(log_cm)

dt_cm= confusion_matrix(y_test, decision_pred)
print(dt_cm)

"""our user-defined confusion matrix calculator works the same as inbuilt"""

inbuilt_acc_logistic = acc(y_test,logistic_pred)
inbuilt_acc_decision = acc(y_test,decision_pred)

print("average Accuracy for Logistic Regression-based Predictions (inbuilt) =>",str(inbuilt_acc_logistic*100)+"%")
print("average Accuracy for Decision Tree-based Predictions (inbuilt)=>",str(inbuilt_acc_decision*100)+"%")
print('\n')

def avg_accuracy(cm):
  '''
  average class accuracy= ( TN+ TP)/ (TN + TP + FN + FP)
  '''
  print(100*(cm[0,0]+cm[1,1])/ (cm[0,0]+ cm[0,1]+ cm[1,0]+ cm[1,1]) )

def class_accuracy(cm):
  '''
  Class wise accuracy= ( TN/ ( TN+ FP)+ TP/( TP + FN))/2
  '''
  sol1= cm[0,0]/( cm[0,0] + cm[0,1])  ##TN/ ( TN+ FP): accuracy of class 1(negative class)
  sol2= cm[1,1]/(cm[1,1]+ cm[1,0])    ##TP/( TP + FN): accuracy of class 2(+ve class)
  print( 100*(sol1+sol2)/2)

inbuilt_ps_logistic = ps(y_test,logistic_pred)
inbuilt_ps_decision = ps(y_test,decision_pred)

print("Precision for Logistic Regression-based Predictions (inbuilt) =>",str(inbuilt_ps_logistic*100)+"%")
print("Precision for Decision Tree-based Predictions (inbuilt)=>",str(inbuilt_ps_decision*100)+"%")
print('\n')

def precision(cm):
  ## TP/( TP +FP)
  print( 100*cm[1,1]/( cm[1,1]+ cm[0,1]))

inbuilt_rs_logistic = rs(y_test,logistic_pred)
inbuilt_rs_decision = rs(y_test,decision_pred)

print("Recall for Logistic Regression-based Predictions (inbuilt)=>",str(inbuilt_rs_logistic*100)+"%")
print("Recall for Decision Tree-based Predictions (inbuilt)=>",str(inbuilt_rs_decision*100)+"%")
print('\n')

def recall(cm):
  ## TP/( TP+ FN)
  print( 100*cm[1,1]/(cm[1,1]+ cm[1,0]))

inbuilt_f1s_logistic = f1s(y_test,logistic_pred)
inbuilt_f1s_decision = f1s(y_test,decision_pred)

print("F1-Score for Logistic Regression-based Predictions(inbuilt) =>",str(inbuilt_f1s_logistic*100)+"%")
print("F1-Score for Decision Tree-based Predictions (inbuilt)=>",str(inbuilt_f1s_decision*100)+"%")
print('\n')

def f1_score(cm):
  ##harmonic mean of recall and precision
  p=cm[1,1]/( cm[1,1]+ cm[0,1])
  r=cm[1,1]/(cm[1,1]+ cm[1,0])
  print( 200*p*r/(p+r))

def sensitivity(cm):
  #proportion of actual +ve cases that were correctly identified, same as recall
  print(100*cm[1,1]/(cm[1,1]+ cm[1,0]))

def specificity(cm):
  #proportion of actual -ve cases that were correctly identified. TN/(TN+ FP)
  print( 100*cm[0,0]/( cm[0,0]+ cm[0,1]))

print('average Accuracy for Logistic Regression-based Predictions')
avg_accuracy(log_cm) 
print('average Accuracy for Decision Tree-based Predictions')
avg_accuracy(dt_cm)
print('\n')

print('Class-wise Accuracy for Logistic Regression-based Predictions')
class_accuracy(log_cm) 
print('class-wise Accuracy for Decision Tree-based Predictions')
class_accuracy(dt_cm)
print('\n')

print('Precision for Logistic Regression-based Predictions')
precision(log_cm)
print('Precision for Decision Tree-based Predictions')
precision(dt_cm)
print('\n')

print('Recall for Logistic Regression-based Predictions')
recall(log_cm)
print('Recall for Decision Tree-based Predictions')
recall(dt_cm)
print('\n')

print('F1-score for Logistic Regression-based Predictions')
f1_score(log_cm)
print('F1-score for Decision Tree-based Predictions')
f1_score(dt_cm)
print('\n')

print('Sensitivity for Logistic Regression-based Predictions')
sensitivity(log_cm)
print('Sensitivity for Decision Tree-based Predictions')
sensitivity(dt_cm)
print('\n')

print('Specificity for Logistic Regression-based Predictions')
specificity(log_cm)
print('Specificity for Decision Tree-based Predictions')
specificity(dt_cm)
print('\n')